{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a31541dc",
   "metadata": {},
   "source": [
    "# Improved Heart Attack Prediction using Neural Networks\n",
    "This notebook preprocesses data, trains an improved neural network, and evaluates its performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a908b710",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81718ce3",
   "metadata": {},
   "source": [
    "## Load and Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d715ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "data_path = \"Resources/heart_attack_predictions.csv\"\n",
    "df = pd.read_csv(data_path)\n",
    "\n",
    "# Display first few rows\n",
    "display(df.head())\n",
    "\n",
    "# Check for missing values\n",
    "print(\"Missing values per column:\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# Fill or drop missing values\n",
    "df = df.dropna()\n",
    "\n",
    "# Convert categorical columns to numerical if needed\n",
    "for column in df.select_dtypes(include=['object']).columns:\n",
    "    le = LabelEncoder()\n",
    "    df[column] = le.fit_transform(df[column])\n",
    "\n",
    "# Assume the last column is the target variable\n",
    "X = df.iloc[:, :-1].values\n",
    "y = df.iloc[:, -1].values  # Assuming it's a binary classification (0 = deceased, 1 = survived)\n",
    "\n",
    "# Handle class imbalance using SMOTE\n",
    "smote = SMOTE(random_state=42)\n",
    "X, y = smote.fit_resample(X, y)\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Normalize features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea2b2091",
   "metadata": {},
   "source": [
    "## Build and Train Improved Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3462c121",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m 4737/11688\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m20s\u001b[0m 3ms/step - accuracy: 0.4957 - loss: 0.6932"
     ]
    }
   ],
   "source": [
    "# Build the improved neural network model\n",
    "model = Sequential([\n",
    "    Dense(256, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.5),\n",
    "    Dense(128, activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.4),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.3),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dropout(0.2),\n",
    "    Dense(1, activation='sigmoid')  # Output layer for binary classification\n",
    "])\n",
    "\n",
    "# Compile model with tuned learning rate\n",
    "optimizer = Adam(learning_rate=0.0005)\n",
    "model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model with early stopping\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "history = model.fit(X_train, y_train, epochs=150, batch_size=32, validation_data=(X_test, y_test), callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3d1da4a",
   "metadata": {},
   "source": [
    "## Evaluate the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "917933f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.title('Model Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Evaluate model\n",
    "y_pred = (model.predict(X_test) > 0.5).astype(int)\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "671c4225",
   "metadata": {},
   "source": [
    "## Save the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbc30114",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "model.save(\"/Resources/heart_attack_model.h5\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
